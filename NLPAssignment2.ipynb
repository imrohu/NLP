{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPAssignment2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imrohu/NLP/blob/master/NLPAssignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaWyY1R3xZS7",
        "colab_type": "code",
        "outputId": "4de218af-4d43-4623-e0ca-3288fc02f1fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import os\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ms9MOZFBNB_",
        "colab_type": "code",
        "outputId": "e067c89b-36cc-4a47-eacf-bcd13e34f3f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import keras \n",
        "import random\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re \n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "url='https://raw.githubusercontent.com/cacoderquan/Sentiment-Analysis-on-the-Rotten-Tomatoes-movie-review-dataset/master/train.tsv'\n",
        "data=pd.read_csv(url, sep='\\t', header=0, quoting=3)\n",
        "\n",
        "data.head(100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage that what is good for the goose</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>A series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>96</td>\n",
              "      <td>3</td>\n",
              "      <td>, I suspect ,</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>97</td>\n",
              "      <td>3</td>\n",
              "      <td>I suspect ,</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>98</td>\n",
              "      <td>3</td>\n",
              "      <td>I suspect</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>99</td>\n",
              "      <td>3</td>\n",
              "      <td>I</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>100</td>\n",
              "      <td>3</td>\n",
              "      <td>suspect</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows Ã— 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    PhraseId  ...  Sentiment\n",
              "0          1  ...          1\n",
              "1          2  ...          2\n",
              "2          3  ...          2\n",
              "3          4  ...          2\n",
              "4          5  ...          2\n",
              "..       ...  ...        ...\n",
              "95        96  ...          2\n",
              "96        97  ...          2\n",
              "97        98  ...          2\n",
              "98        99  ...          2\n",
              "99       100  ...          2\n",
              "\n",
              "[100 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WA83kkjm12C8",
        "colab_type": "code",
        "outputId": "711c9e50-82e0-4054-dca7-e9454d7985bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(156060, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQWAqeYZCkna",
        "colab_type": "code",
        "outputId": "15eec8c6-eb14-4ed9-b455-97d895415fd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "import nltk\n",
        "import random\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "pd.set_option('max_colwidth',400)\n",
        "\n",
        "wordnet = WordNetLemmatizer()\n",
        "stopwords_en = stopwords.words(\"english\")\n",
        "punctuations = \"?:!.,;-()\"\n",
        "\n",
        "raw_reviews = data.Phrase.values\n",
        "cleaned_reviews = []\n",
        "\n",
        "for i in range(len(raw_reviews)):\n",
        "  review = str(raw_reviews[i])\n",
        "  review=re.sub('[^a-zA-Z]',' ',review)\n",
        "  review=[wordnet.lemmatize(w) for w in word_tokenize(str(review).lower())]\n",
        "  review=' '.join(review)\n",
        "  cleaned_reviews.append(review)\n",
        "\n",
        "data['cleaned_reviews'] = cleaned_reviews\n",
        "\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>cleaned_reviews</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .</td>\n",
              "      <td>1</td>\n",
              "      <td>a series of escapade demonstrating the adage that what is good for the goose is also good for the gander some of which occasionally amuses but none of which amount to much of a story</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage that what is good for the goose</td>\n",
              "      <td>2</td>\n",
              "      <td>a series of escapade demonstrating the adage that what is good for the goose</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>A series</td>\n",
              "      <td>2</td>\n",
              "      <td>a series</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "      <td>series</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PhraseId  ...                                                                                                                                                                         cleaned_reviews\n",
              "0         1  ...  a series of escapade demonstrating the adage that what is good for the goose is also good for the gander some of which occasionally amuses but none of which amount to much of a story\n",
              "1         2  ...                                                                                                            a series of escapade demonstrating the adage that what is good for the goose\n",
              "2         3  ...                                                                                                                                                                                a series\n",
              "3         4  ...                                                                                                                                                                                       a\n",
              "4         5  ...                                                                                                                                                                                  series\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVtomZUZFnd_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "X = data.cleaned_reviews.values\n",
        "Y = to_categorical(data.Sentiment.values)\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words = None, ngram_range=(1,2))\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.3, random_state=2003)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78siFCw6FsJr",
        "colab_type": "code",
        "outputId": "07f16a46-90dc-46b9-ea7b-7efa4906cdc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from nltk import FreqDist\n",
        "all_words=' '.join(X_train)\n",
        "all_words=word_tokenize(all_words)\n",
        "dist=FreqDist(all_words)\n",
        "\n",
        "num_unique_word=len(dist)\n",
        "num_unique_word"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13725"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfaTBPJSFsIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import torch\n",
        "torch.manual_seed(2020)\n",
        "from torch.nn import Conv1d\n",
        "from torch.nn import MaxPool1d\n",
        "from torch.nn import Flatten\n",
        "from torch.nn import Linear\n",
        "from torch.nn.functional import relu, softmax, sigmoid\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.optim import SGD, Adam\n",
        "from torch.nn import L1Loss, CrossEntropyLoss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJ_R-FtYGW7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class CnnClassifier(torch.nn.Module):\n",
        "  def __init__(self, batch_size, inputs, outputs):\n",
        "\n",
        "    super(CnnClassifier, self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.inputs = inputs\n",
        "    self.outputs = outputs\n",
        "    self.input_layer = Conv1d(inputs, batch_size, 1)\n",
        "    self.max_pooling_layer = MaxPool1d(1)\n",
        "    self.conv_layer = Conv1d(batch_size, 128, 1)\n",
        "    self.flatten_layer = Flatten()\n",
        "    self.linear_layer = Linear(128, 64)\n",
        "    self.output_layer = Linear(64, outputs)\n",
        "  def feed(self, input):\n",
        "    input = input.reshape((self.batch_size, self.inputs, 1))\n",
        "    output = relu(self.input_layer(input))\n",
        "\n",
        "    output = self.max_pooling_layer(output)\n",
        "\n",
        "    output = relu(self.conv_layer(output))\n",
        "\n",
        "    output = self.flatten_layer(output)\n",
        "\n",
        "    output = self.linear_layer(output)\n",
        "\n",
        "    output = self.output_layer(output)\n",
        "  \n",
        "    output = softmax(output)\n",
        "   \n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79XIb_jkygpm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def categorical_accuracy(preds, y):\n",
        "  \n",
        "    max_preds = preds.argmax(dim = 1, keepdim = True)\n",
        "    correct = max_preds.squeeze(1).eq(y)\n",
        "    return (correct.sum() / torch.FloatTensor([y.shape[0]])) / torch.FloatTensor([y.shape[0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp70eeda_POS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def model_loss(model, dataset, train = False, optimizer = None):\n",
        "\n",
        "  performance = L1Loss()\n",
        "  criterion = CrossEntropyLoss()\n",
        "  avg_loss = 0\n",
        "  avg_accu = 0\n",
        "  count = 0\n",
        "  \n",
        "  for input, output in iter(dataset):\n",
        "    predictions = model.feed(input)\n",
        "    \n",
        "    loss = performance(predictions, output)\n",
        "    \n",
        "    tmp_accu = categorical_accuracy(predictions, output)\n",
        "    \n",
        "    if(train):\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    avg_loss += loss.item()\n",
        "\n",
        "    avg_accu += tmp_accu.item()\n",
        "    count += 1\n",
        "    \n",
        "  return avg_loss / count, avg_accu / count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wg2pZMbWmL3T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "666a7d21-d2f3-4ed9-d633-0ff23ea52257"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mChSvtCVd4XB",
        "colab_type": "code",
        "outputId": "5e198b5d-f15b-42df-e056-9863f85e13a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "batch_size = 128\n",
        "\n",
        "x_train = vectorizer.fit_transform(X_train)\n",
        "model = CnnClassifier(batch_size, x_train.shape[1], 5)\n",
        "\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CnnClassifier(\n",
              "  (input_layer): Conv1d(88931, 128, kernel_size=(1,), stride=(1,))\n",
              "  (max_pooling_layer): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv_layer): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
              "  (flatten_layer): Flatten()\n",
              "  (linear_layer): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (output_layer): Linear(in_features=64, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6LuCuZZF1yB",
        "colab_type": "code",
        "outputId": "558d4aa8-ef36-4d7c-cb96-d063a436762f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "r_len=[]\n",
        "for text in X_train:\n",
        "    word=word_tokenize(text)\n",
        "    l=len(word)\n",
        "    \n",
        "    r_len.append(l)\n",
        "    \n",
        "MAX_REVIEW_LEN=np.max(r_len)\n",
        "MAX_REVIEW_LEN\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5jmZpaAF3-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_features = num_unique_word\n",
        "max_words = MAX_REVIEW_LEN\n",
        "batch_size = 256\n",
        "epochs=5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "239huopwGHSD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense, GRU, Embedding\n",
        "from tensorflow.python.keras.optimizers import Adam\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_val = tokenizer.texts_to_sequences(X_val)\n",
        "\n",
        "X_test = tokenizer.texts_to_sequences(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DU_TqswGUT5",
        "colab_type": "code",
        "outputId": "ca6d4c7f-9602-4cc1-8264-3857a202bc98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "from keras.preprocessing import sequence,text\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Input, Dense, Embedding, Flatten\n",
        "from keras.layers import SpatialDropout1D\n",
        "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
        "from keras.models import Sequential\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
        "X_val = sequence.pad_sequences(X_val, maxlen=max_words)\n",
        "X_val = sequence.pad_sequences(X_val, maxlen=max_words)\n",
        "print(X_train.shape,X_val.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(109242, 48) (46818, 48)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmUuhzYIGY4C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2 = Sequential()\n",
        "\n",
        "model2.add(Embedding(max_features, 150, input_length=max_words))\n",
        "\n",
        "model2.add(SpatialDropout1D(0.2))\n",
        "\n",
        "model2.add(Conv1D(32, kernel_size=3, padding='same', activation='relu'))\n",
        "model2.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "model2.add(Conv1D(64, kernel_size=3, padding='same', activation='relu'))\n",
        "model2.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "model2.add(Flatten())\n",
        "\n",
        "model2.add(Dense(5, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiF8QFwDEO7B",
        "colab_type": "code",
        "outputId": "c6fbb690-c726-4a80-ae32-eafe65098317",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "torch.save(model, '/content/gdrive/My Drive/Assignment2.pt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type CnnClassifier. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WTPkJYQE_lg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = torch.load('/content/gdrive/My Drive/Assignment2.pt')\n",
        "#model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTi6NNzWGdOk",
        "colab_type": "code",
        "outputId": "2f003f99-db97-4ae0-e130-d87b5b3ecdba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model2.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=epochs, batch_size=batch_size, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 109242 samples, validate on 46818 samples\n",
            "Epoch 1/5\n",
            "109242/109242 [==============================] - 3s 30us/step - loss: 1.2922 - acc: 0.5089 - val_loss: 1.2825 - val_acc: 0.5123\n",
            "Epoch 2/5\n",
            "109242/109242 [==============================] - 3s 26us/step - loss: 1.2865 - acc: 0.5089 - val_loss: 1.2817 - val_acc: 0.5123\n",
            "Epoch 3/5\n",
            "109242/109242 [==============================] - 3s 27us/step - loss: 1.2862 - acc: 0.5089 - val_loss: 1.2824 - val_acc: 0.5123\n",
            "Epoch 4/5\n",
            "109242/109242 [==============================] - 3s 27us/step - loss: 1.2860 - acc: 0.5089 - val_loss: 1.2829 - val_acc: 0.5123\n",
            "Epoch 5/5\n",
            "109242/109242 [==============================] - 3s 26us/step - loss: 1.2858 - acc: 0.5089 - val_loss: 1.2824 - val_acc: 0.5123\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb64e8cc710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    }
  ]
}