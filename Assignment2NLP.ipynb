{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2NLP.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imrohu/NLP/blob/master/Assignment2NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3B6uba74tMZS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "28611aab-dd10-4875-9ae0-b8707838f257"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import os\n",
        "\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxJoRsoFq2Ys",
        "colab_type": "code",
        "outputId": "1ccc40a2-363b-4e51-fc10-ffc24198c487",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import keras \n",
        "import random\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re \n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "url='https://raw.githubusercontent.com/cacoderquan/Sentiment-Analysis-on-the-Rotten-Tomatoes-movie-review-dataset/master/train.tsv'\n",
        "data=pd.read_csv(url, sep='\\t', header=0, quoting=3)\n",
        "\n",
        "data.head(100)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage that what is good for the goose</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>A series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>96</td>\n",
              "      <td>3</td>\n",
              "      <td>, I suspect ,</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>97</td>\n",
              "      <td>3</td>\n",
              "      <td>I suspect ,</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>98</td>\n",
              "      <td>3</td>\n",
              "      <td>I suspect</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>99</td>\n",
              "      <td>3</td>\n",
              "      <td>I</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>100</td>\n",
              "      <td>3</td>\n",
              "      <td>suspect</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows Ã— 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    PhraseId  ...  Sentiment\n",
              "0          1  ...          1\n",
              "1          2  ...          2\n",
              "2          3  ...          2\n",
              "3          4  ...          2\n",
              "4          5  ...          2\n",
              "..       ...  ...        ...\n",
              "95        96  ...          2\n",
              "96        97  ...          2\n",
              "97        98  ...          2\n",
              "98        99  ...          2\n",
              "99       100  ...          2\n",
              "\n",
              "[100 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEc8UK44tS2B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fabdbba0-877f-492f-c58f-7d3c13b85dd0"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(156060, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pEHC3i0rK5Q",
        "colab_type": "code",
        "outputId": "47cf2ff4-8092-4a8c-c0a6-d06a6416d4ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re \n",
        "\n",
        "pd.set_option('max_colwidth',400)\n",
        "\n",
        "wordnet = WordNetLemmatizer()\n",
        "stopwords_en = stopwords.words(\"english\")\n",
        "punctuations = \"?:!.,;-()\"\n",
        "\n",
        "raw_reviews = data.Phrase.values\n",
        "cleaned_reviews = []\n",
        "\n",
        "for i in range(len(raw_reviews)):\n",
        "  review = str(raw_reviews[i])\n",
        "  review=re.sub('[^a-zA-Z]',' ',review)\n",
        "  review=[wordnet.lemmatize(w) for w in word_tokenize(str(review).lower())]\n",
        "  review=' '.join(review)\n",
        "  cleaned_reviews.append(review)\n",
        "\n",
        "data['cleaned_reviews'] = cleaned_reviews\n",
        "\n",
        "data.head()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>cleaned_reviews</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .</td>\n",
              "      <td>1</td>\n",
              "      <td>a series of escapade demonstrating the adage that what is good for the goose is also good for the gander some of which occasionally amuses but none of which amount to much of a story</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage that what is good for the goose</td>\n",
              "      <td>2</td>\n",
              "      <td>a series of escapade demonstrating the adage that what is good for the goose</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>A series</td>\n",
              "      <td>2</td>\n",
              "      <td>a series</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "      <td>series</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PhraseId  ...                                                                                                                                                                         cleaned_reviews\n",
              "0         1  ...  a series of escapade demonstrating the adage that what is good for the goose is also good for the gander some of which occasionally amuses but none of which amount to much of a story\n",
              "1         2  ...                                                                                                            a series of escapade demonstrating the adage that what is good for the goose\n",
              "2         3  ...                                                                                                                                                                                a series\n",
              "3         4  ...                                                                                                                                                                                       a\n",
              "4         5  ...                                                                                                                                                                                  series\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsZMh4qCr9mE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "X = data.cleaned_reviews.values\n",
        "Y = to_categorical(data.Sentiment.values)\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words = None, ngram_range=(1,2))\n",
        "#X = vectorizer.fit_transform(X).toarray()\n",
        "data1 = data.cleaned_reviews.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3fxFWLZsonK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.3, random_state=2003)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk-jhTBWtaGM",
        "colab_type": "code",
        "outputId": "c813350d-bf84-4eff-95a5-cd50cb378847",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from nltk import FreqDist\n",
        "all_words=' '.join(x_train)\n",
        "all_words=word_tokenize(all_words)\n",
        "dist=FreqDist(all_words)\n",
        "\n",
        "num_unique_word=len(dist)\n",
        "num_unique_word"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13725"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQZZyTa2wSX-",
        "colab_type": "code",
        "outputId": "c243bca2-f13c-4e06-9c68-43bbae0757a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "r_len=[]\n",
        "for text in x_train:\n",
        "    word=word_tokenize(text)\n",
        "    l=len(word)\n",
        "\n",
        "    r_len.append(l)\n",
        "    \n",
        "MAX_REVIEW_LEN=np.max(r_len)\n",
        "MAX_REVIEW_LEN"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77dRDxuywbsO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_features = num_unique_word\n",
        "max_words = MAX_REVIEW_LEN\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "num_classes=5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvT-UZqQydHo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense, GRU, Embedding\n",
        "from tensorflow.python.keras.optimizers import Adam\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "x_train = tokenizer.texts_to_sequences(x_train)\n",
        "x_val = tokenizer.texts_to_sequences(x_val)\n",
        "\n",
        "x_test = tokenizer.texts_to_sequences(data1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLifVg7-yltj",
        "colab_type": "code",
        "outputId": "650db446-1d67-4b50-d8c1-b65d7e9a35af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "from keras.preprocessing import sequence,text\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=max_words)\n",
        "x_val = sequence.pad_sequences(x_val, maxlen=max_words)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=max_words)\n",
        "#print(X_train.shape,X_val.shape)\n",
        "x_test"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0, ...,    3,    2,   39],\n",
              "       [   0,    0,    0, ...,   14,    1, 3134],\n",
              "       [   0,    0,    0, ...,    0,    2,  338],\n",
              "       ...,\n",
              "       [   0,    0,    0, ...,    0, 8655, 7961],\n",
              "       [   0,    0,    0, ...,    0,    0, 8655],\n",
              "       [   0,    0,    0, ...,    0,    0, 7961]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPKUv7ZFwlWH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Input, Dense, Embedding, Flatten\n",
        "from keras.layers import SpatialDropout1D\n",
        "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
        "from keras.models import Sequential"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_K4STsXwzxZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2 = Sequential()\n",
        "\n",
        "# Input / Embdedding\n",
        "model2.add(Embedding(max_features, 200, input_length=max_words))\n",
        "\n",
        "# CNN\n",
        "model2.add(SpatialDropout1D(0.2))\n",
        "\n",
        "model2.add(Conv1D(64, kernel_size=3, padding='same', activation='relu'))\n",
        "model2.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "model2.add(Conv1D(128, kernel_size=3, padding='same', activation='relu'))\n",
        "model2.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "model2.add(Flatten())\n",
        "\n",
        "# Output layer\n",
        "model2.add(Dense(4, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__a_V_dGrwe4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as Ke\n",
        "def recall_nlp(true, pred):\n",
        "    t_p = Ke.sum(Ke.round(Ke.clip(true * pred, 0, 1)))\n",
        "    p_p = Ke.sum(Ke.round(Ke.clip(true, 0, 1)))\n",
        "    recall = t_p/ (p_p + Ke.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_nlp(true, pred):\n",
        "    t_p = Ke.sum(Ke.round(Ke.clip(true * pred, 0, 1)))\n",
        "    p_p = Ke.sum(Ke.round(Ke.clip(pred, 0, 1)))\n",
        "    precision = t_p / (p_p + Ke.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_nlp(true, pred):\n",
        "    precision = precision_nlp(true, pred)\n",
        "    recall = recall_nlp(true, pred)\n",
        "    return 2*((precision*recall)/(precision+recall+Ke.epsilon()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxGP4RV_w5AR",
        "colab_type": "code",
        "outputId": "ffac07ec-6a74-4823-936d-2355d70519e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "from keras import metrics\n",
        "from sklearn.metrics import f1_score\n",
        "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[metrics.categorical_accuracy,f1_nlp,precision_nlp, recall_nlp])\n",
        "model2.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=epochs, batch_size=batch_size, verbose=1)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 109242 samples, validate on 46818 samples\n",
            "Epoch 1/10\n",
            "109242/109242 [==============================] - 11s 96us/step - loss: 1.0165 - categorical_accuracy: 0.5915 - f1_nlp: 0.5410 - precision_nlp: 0.6652 - recall_nlp: 0.4626 - val_loss: 0.8624 - val_categorical_accuracy: 0.6459 - val_f1_nlp: 0.6169 - val_precision_nlp: 0.6906 - val_recall_nlp: 0.5578\n",
            "Epoch 2/10\n",
            "109242/109242 [==============================] - 10s 89us/step - loss: 0.7788 - categorical_accuracy: 0.6834 - f1_nlp: 0.6653 - precision_nlp: 0.7201 - recall_nlp: 0.6187 - val_loss: 0.8330 - val_categorical_accuracy: 0.6569 - val_f1_nlp: 0.6315 - val_precision_nlp: 0.6982 - val_recall_nlp: 0.5768\n",
            "Epoch 3/10\n",
            "109242/109242 [==============================] - 10s 89us/step - loss: 0.6867 - categorical_accuracy: 0.7168 - f1_nlp: 0.7061 - precision_nlp: 0.7468 - recall_nlp: 0.6699 - val_loss: 0.8419 - val_categorical_accuracy: 0.6602 - val_f1_nlp: 0.6462 - val_precision_nlp: 0.6855 - val_recall_nlp: 0.6115\n",
            "Epoch 4/10\n",
            "109242/109242 [==============================] - 10s 89us/step - loss: 0.6157 - categorical_accuracy: 0.7444 - f1_nlp: 0.7368 - precision_nlp: 0.7697 - recall_nlp: 0.7068 - val_loss: 0.8655 - val_categorical_accuracy: 0.6604 - val_f1_nlp: 0.6498 - val_precision_nlp: 0.6821 - val_recall_nlp: 0.6207\n",
            "Epoch 5/10\n",
            "109242/109242 [==============================] - 10s 88us/step - loss: 0.5619 - categorical_accuracy: 0.7657 - f1_nlp: 0.7611 - precision_nlp: 0.7899 - recall_nlp: 0.7346 - val_loss: 0.9130 - val_categorical_accuracy: 0.6545 - val_f1_nlp: 0.6456 - val_precision_nlp: 0.6722 - val_recall_nlp: 0.6213\n",
            "Epoch 6/10\n",
            "109242/109242 [==============================] - 10s 91us/step - loss: 0.5188 - categorical_accuracy: 0.7837 - f1_nlp: 0.7798 - precision_nlp: 0.8050 - recall_nlp: 0.7562 - val_loss: 0.9714 - val_categorical_accuracy: 0.6537 - val_f1_nlp: 0.6460 - val_precision_nlp: 0.6677 - val_recall_nlp: 0.6259\n",
            "Epoch 7/10\n",
            "109242/109242 [==============================] - 10s 89us/step - loss: 0.4796 - categorical_accuracy: 0.7986 - f1_nlp: 0.7954 - precision_nlp: 0.8171 - recall_nlp: 0.7750 - val_loss: 0.9926 - val_categorical_accuracy: 0.6477 - val_f1_nlp: 0.6398 - val_precision_nlp: 0.6642 - val_recall_nlp: 0.6174\n",
            "Epoch 8/10\n",
            "109242/109242 [==============================] - 10s 89us/step - loss: 0.4497 - categorical_accuracy: 0.8125 - f1_nlp: 0.8101 - precision_nlp: 0.8300 - recall_nlp: 0.7913 - val_loss: 1.0617 - val_categorical_accuracy: 0.6397 - val_f1_nlp: 0.6339 - val_precision_nlp: 0.6536 - val_recall_nlp: 0.6154\n",
            "Epoch 9/10\n",
            "109242/109242 [==============================] - 10s 90us/step - loss: 0.4203 - categorical_accuracy: 0.8236 - f1_nlp: 0.8216 - precision_nlp: 0.8394 - recall_nlp: 0.8046 - val_loss: 1.1160 - val_categorical_accuracy: 0.6371 - val_f1_nlp: 0.6308 - val_precision_nlp: 0.6492 - val_recall_nlp: 0.6136\n",
            "Epoch 10/10\n",
            "109242/109242 [==============================] - 10s 89us/step - loss: 0.3974 - categorical_accuracy: 0.8345 - f1_nlp: 0.8334 - precision_nlp: 0.8495 - recall_nlp: 0.8180 - val_loss: 1.1818 - val_categorical_accuracy: 0.6403 - val_f1_nlp: 0.6357 - val_precision_nlp: 0.6508 - val_recall_nlp: 0.6214\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2bddfc3d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8OFLCNE2Vjc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e64c8f03-32cd-4abf-c017-380a3790948d"
      },
      "source": [
        "loss, accuracy, f1_score, precision, recall = model2.evaluate(x_val, y_val, verbose=0)\n",
        "print(\"Accuracy \" + str(accuracy) + \":\\n\\ff1_score = \" + str(f1_score) +\n",
        "          \"\\nPrecision = \" + str(precision) + \"\\nRecall = \" + str(recall))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy 0.6403092827544962:\n",
            "\ff1_score = 0.6355485329665064\n",
            "Precision = 0.6508624560708008\n",
            "Recall = 0.6214276560297322\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XH9CB6Vs-j_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2.save(\"1107345.pt\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}